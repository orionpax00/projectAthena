{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fe2ocTS5xziD"
   },
   "source": [
    "# Attention Mechanism ðŸŽ¯\n",
    "\n",
    "Attention Mechanism as the name indicated is similiar to the it actual Vocab meaning that is *the action of dealing with or taking special care of someone or something*. In deep learning problems where model have to check for the state before the currents state of the it the problem of not remembering the complete information from the initial step when the number of steps are too many, similiar in image recognition there are some areas of image that are more important then the rest so, paying more attention to them will effect the results significantly, most common example are Neural machine translation and Image Captioning.\n",
    "\n",
    "### Lets Take One example from each category...\n",
    "* ### Image Captioning\n",
    "Image captioning is a method which enables a model to see an image prediction a single sentence about whats there in the Image.\n",
    "Label the below image so, whats you answer,<br>\n",
    "*A dog with a ball* **Type I Interpretation**<br>\n",
    "*A dog playing with ball* **Type II Interpretation**<br>\n",
    "*A dog is trying to catch a ball with his mouth in a grass land* **Type III Interpretation**\n",
    "<table border=\"0\" bordercolor=\"red\" align=\"center\">\n",
    "    <tr>\n",
    "        <th><img src=\"./assets/img/original_dog_ball.jpg\" width=400></th>\n",
    "    </tr>\n",
    "<table>\n",
    "So the now question arises how did human brain understand that information, well that happen piece by piece first your brain extracted that there is dog in it, that is it focus on dog and then on ball and then what else is happening in image.\n",
    "<table border=\"0\" bordercolor=\"red\" align=\"center\">\n",
    "    <tr>\n",
    "        <th><img src=\"./assets/img/original_dog_ball_focussed.jpg\" width=400></th>\n",
    "    </tr>\n",
    "<table>\n",
    "    \n",
    "\n",
    "### Resources\n",
    "[1] https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html <br>\n",
    "[2] https://www.tensorflow.org/tutorials/text/image_captioning <br>\n",
    "[3] https://www.tensorflow.org/tutorials/text/nmt_with_attention <br>\n",
    "[4] https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdaXzBsBxsyG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "attention_ntm_dnc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
